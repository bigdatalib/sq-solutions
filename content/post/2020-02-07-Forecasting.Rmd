---
title: "Commodities Forecasting"
author: "Nima Safaian"
date: 2020-02-07T23:41:14-05:00
draft: false
categories: ["Data"]
tags: ["Data", "Analytics","Strategy"]
thumbnailImagePosition: left
thumbnailImage: ./images/logo.png
metaAlignment: center
disable_comments: true
output:
  blogdown::html_page:
    toc: false
    fig_width: 8
    css: "/css/my-style.css"
---

This blog post is in part, designed for a presentation to MRU business students in Calgary and covers 3 concepts:

+ Models and Why we need them? Supply/Demand Models
+ Short Term forcasting (1) Time Series and (2) Regressive Forecast
+ Long term forecasts

The content is based on two main sources:

(1) [Time Series Analysis with R by Rami Krispin](https://www.amazon.ca/Hands-Time-Analysis-forecasting-visualization-ebook/dp/B07B41P2HZ)
(2) [Forecasting Principles and Practive - Rob J Hyndman and George Athanasopoulos](https://otexts.com/fpp3/)

## Models why do we need them?

### Model Thinker and Many Model Approach

*"Models are formal structures represented in mathematics and diagrams that help us to understand the world. Mastery of models improves your ability to reason, explain, design, communicate, act, predict and explore - The Model Thinker - Scott E. Page"* 

**Models are simply used to simplify complexity**

```{r, echo=FALSE,fig.align="center",out.width="50%",fig.cap="The Model Thinker by - Scott E. Page"}
knitr::include_graphics(c("https://bigdatalibopen.s3-us-west-2.amazonaws.com/blog/modelThinker.png"))
```

+ Models fall into general categories: 

  1. Simplification of the world (e.g. Simple supply demand economic models).
  2. Mathematical Analogies (e.g. Statistical Inference).
  3. Exploratory Constructs (e.g. Outlier detection of complex data set).
+ Many Model Approach vs. Single Model Approach.
+ Models to Explain vs. Models to Predict (Beta hat vs. Y hat problems).

## Analytics in the Data Age 

*T. S. Eliot: “Where is the wisdom we have lost in knowledge? Where is the knowledge we have lost in information?”* 

```{r, echo=FALSE,fig.align="center",out.width="50%",fig.cap="The Model Thinker by - Scott E. Page"}
knitr::include_graphics(c("https://bigdatalibopen.s3-us-west-2.amazonaws.com/blog/wisdomchart.png"))

```
  

### Skills needed to transform market data into trading wisdom

**Data**

+ Efficiency, Automation and Creativity.  
+ Curating the right data is the most time consuming part of process.
+ Establishing pipelines to many data-sets is key.
+ Agile approach to data - Use what you need but automate as you go.

**Information**  

+ Data wrangling - 80/20 rule of data analysis.
+ Basic Statistics (mean,vol,skew,distribution).
+ Exploratory Data Visualization.

**Knowledge**

+ Model building (from easy SD Models to Machine-learning)
+ Hypothesis Testing and Statistical Inference
+ Visual Analytic and Knowledge Sharing  

**Wisdom** 

+ Turning models into action.
+ Knowing where to use which models.
+ Understanding the boundaries of models.

## Supply and Demand Models

### Why S/D

+ Understand Market Structure and Cuasality
+ Understand the Present
+ Forecast short term future. Congestion and shortage
+ Model for Scenario Analysis

### Elements of S/D 

+ Sources (Production)
+ Sinks (Usage and Transformations)
+ Movement (Storage and Transportations)
+ Price Setting Methods and What is Marginal


```{r echo=FALSE}
require("visNetwork")
nodes<-dplyr::tibble(id=1:3,
                  label=c("input","market","output"),
                  group=c("outside","inside","outside"),
                  value=c(1,5,1),
                  shape=c("star","database","circle"),
                  color=c("red","yellow","blue"),
                  shadow=c(T,T,T)
                  )
edges<-dplyr::tibble(from=c(1,2,2),to=c(2,3,2),label=c("In","Out","Gen&Usage"),color=c("grey","grey","grey"))
visNetwork(nodes, edges, height = "500px", width = "100%") %>%
    visEdges(arrows = "to") 
```




## Modelling for Short Term Dynamics

### Sourcing and Visulizing Data 

Using EIA package developed by Matthew Leonawicz we can source data from [Energy Information Administration](https://www.eia.gov/). The package is on cran. 

![](https://docs.ropensci.org/eia/reference/figures/logo.png){ width=20% }

## Forecasting 

+ (1) Do we have forward looking data (intel)
+ (2) Do we have foreward looking predictors that can explain the future state of our desired parameter (weather to forecast gas demand)
+ (3) Do we expect significant regime change in our data (market dynamic changing due to infrastucture)
+ (4) Does the underlying time series have "markov properties" (i.e. asset prices)


Install from cran with

```{r eval=FALSE, include=TRUE}
install.packages("eia")
```

```{r eval=TRUE, include=FALSE}
library(eia)
# not run
eia_set_key("83307AB94D4FC78A247EF9E5FAE67581") # set API key if not already set globally
```


Sourcing data from EIA for Natural Gas Toy example
```{r}
industrialgasusage<-eia::eia_series("NG.N3020US2.M")
residentailgasusage<-eia::eia_series("NG.N3010US2.M")
totalgasusage<-eia::eia_series("NG.N9140US2.M")
```

Total Monthly Gas Consumption in US  

```{r eval=TRUE, include=FALSE}
require(magrittr)
require(ggplot2)
totalgasusage %<>% tidyr::unnest(data) %>% dplyr::select(date,value,year,month) %>% 
#  dplyr::mutate(value=value/lubridate::days_in_month(month)) %>%
  dplyr::mutate(value=value/1000)
totalgasusage %>% ggplot(aes(x=date,y=value))+geom_line()+xlab("BCF")
```

###Time Series Forecasting

The content in blog post are based on the examples provided in Time Series Analysis with R:


![book](https://images-na.ssl-images-amazon.com/images/I/510CG2XCGBL._SX260_.jpg)


This fantastic intro book by Rami Krispin. 


```{r eval=FALSE, include=FALSE}
install.packages("TSstudio")
```

#### Time Series Decomposition and seasonality
```{r fff, message=FALSE, warning=FALSE}
require(forecast)
require(TSstudio)
require(dplyr)
require(tseries)
totalgasusage<-totalgasusage %>% dplyr::arrange(-desc(date))
totalgasusagets <- ts(totalgasusage$value, start=c(2001, 1), end=c(2019, 11), frequency=12)
```

Summary of the data.

```{r message=FALSE, warning=FALSE}
ts_info(totalgasusagets)
```

```{r message=FALSE, warning=FALSE}
ts_plot(totalgasusagets,
        title = "US Monthly NG consumption",
        Xtitle="BCF",
        Xgrid=T,
        Ygrid=T
        )
```

Two types of seasonality pattens.

+ Single Seasonal Pattern - one factor dominates
+ Multiple Seasonal Pattern - could be occilationsa + seasonal pattern or in the case of electricity hourly seasonality and daily and monthly seasonality 

```{r message=FALSE, warning=FALSE}
library(forecast)
ts_seasonal(totalgasusagets,type="normal")

```

Another way to look at this using combination of the 

```{r fig.height=8, fig.width=14}
ts_seasonal(totalgasusagets,type="all")

```


Seasonal decomposition is another way to better understand pattens in the data. There are generally 3-4 components that can reconstitude the original series these are: Trend, Seasoan, Cycle and Irregular values. T/S/C/I.

These could be additive:

$$Y=T+S+C+I$$


or multiplicative:

$$Y=T*S*C*I$$


In most forecasting methods the variation in the series remain constant over time which is the case for additive series. However, sometimes the variation increase with time this where we need apply transformation to the data. Popular transformations are log or box-cox.


```{r message=FALSE, warning=FALSE}
totalgasusagets_lambda=BoxCox.lambda(totalgasusagets)
df<-BoxCox(totalgasusagets,lambda = totalgasusagets_lambda)
ts_plot(df)
```

```{r message=FALSE, warning=FALSE}
df<-decompose(totalgasusagets)
m<-plot(df)
m
```

#### Auto Regressive Models

+ Split data into training and testing
+ train model using training 
+ Evaluate model statistics using both training and testing
+ tune model if possible 
+ modify model paramters and choose a different model
+ train and forecast using the full data set


Splitting

```{r message=FALSE, warning=FALSE}
totalgasusagetspartition<-ts_split(totalgasusagets,sample.out = 12)
train<-totalgasusagetspartition$train
test<-totalgasusagetspartition$test
```

Forecasting

```{r message=FALSE, warning=FALSE}
m<-auto.arima(train)
```

#### Forcast evaluation

+ Residual Analysis - Quality of the model + fitted values in the training partition
+ Score forecast - How well model forecasts values

residual : $$e = Y_t- \bar{Y}_t$$

What we are looking for:

+ Residuals are systematically higher/lower than zero
+ Random spike - potential outliers
+ Residual auto correlation - Seasonal pattern that is not captured by the model
+ Residual distribution - need to be generally normal and spread shows the potential error in the model


```{r message=FALSE, warning=FALSE}
checkresiduals(m)
```


Scoring the forecast: 

+ Mean Squared Error (MSE)
+ Root mean squared error (RMSE)
+ Mean absolute error (MAE)
+ Mean absolute Percentage error (MAPE)

```{r message=FALSE, warning=FALSE}
f<-forecast(m,h=12)
```

Accuracy is a relative metric what we are interested here is to understand if we get fairly consistent accuracy between training and test set. Overall 3% for MAPE is fairly good

```{r message=FALSE, warning=FALSE}
accuracy(f,test)
```

Now putting it all together we can see actual vs. forecast and fitted data to do visual inspection of the results.

```{r message=FALSE, warning=FALSE}
test_forecast(actual=totalgasusagets,forecast.obj = f,test = test)
```

Another way to assess the quality of the forecast to look at how much of variation in the forecast result can be explained by the model vs. mean of the data set.


```{r message=FALSE, warning=FALSE}
meansvalue<-naive(train,n=12)
accuracy(meansvalue,test)
```

Now finalize forecast 

```{r message=FALSE, warning=FALSE}
m_final<-auto.arima(totalgasusagets)
f_final<-forecast(m_final,h=12)
```


```{r message=FALSE, warning=FALSE}
plot_forecast(f_final,
              title="US Natural Gas Consumption forecast",
              Xtitle = "Year",
              Ytitle = "BCF"
              
              )
```

We can use forecasting models to simulate values.

```{r message=FALSE, warning=FALSE}
fc_sim<-forecast_sim(model=m_final,h=50,n=100)
```

### Multiple forecasting models 

We can also compare different forecasting methods  

https://cran.r-project.org/web/packages/forecastHybrid/vignettes/forecast Hybrid.html

```{r message=FALSE, warning=FALSE}
require(forecastHybrid)
hm1 <- hybridModel(y = totalgasusagets, models = "aefnstz", weights = "equal", errorMethod = "MASE")
```

```{r message=FALSE, warning=FALSE}
accuracy(hm1,individual = T)
```

### Regression Forecasting

The final approach to forecasting is regression forecasting. The general formulation for regression (linear) looks like:


$$Y=B_0+B_1X_1+B_2X_2+B_3X_3...+B_nX_n+e$$

The most common estimation is OLS. 

The assumptions in OLS

+ the model coefficients follow a linear structure
+ No prefect col-linearity - one variable can not be linear combination of others
+ Independent variable should not have zero variance
+ error term is a "normal distribution" with mu=0
+ both independent and dependent variables draw from the population in a random sample



```{r message=FALSE, warning=FALSE}
df<-decompose(totalgasusagets)

seasonal<-timetk::tk_tbl(df$seasonal,rename_index="date") %>% dplyr::rename(seasonal=value)
trend<-timetk::tk_tbl(df$trend,rename_index="date")%>% dplyr::rename(trend=value)
values<-timetk::tk_tbl(df$x,rename_index="date") %>% dplyr::rename(values=value)

all<-values %>% dplyr::left_join(seasonal,by="date") %>%
  dplyr::left_join(trend,by="date") %>% na.omit()
```

```{r message=FALSE, warning=FALSE}
m<-lm(values~seasonal+trend,data=all[1:200,])
summary(m)
```

```{r message=FALSE, warning=FALSE}
f<-forecast(m,newdata = all[200:215,],h = 20)
f
```

```{r message=FALSE, warning=FALSE}

nall<-data.frame(f) %>% dplyr::rename(values=`Point.Forecast`)
a<-all %>% dplyr::select(values) %>% bind_rows(nall) %>%
  dplyr::select(values) %>% tibble::rowid_to_column()

a %>% ggplot()+geom_line(aes(x=rowid,y=values,color="actual"),data=a[1:215,])+geom_line(aes(x=rowid,y=values,color="forecast"),data=a[216:231,])

```

While this method can seem counter intuitive and containing more steps but it provide a great deal of flexibility around independent variables. Forexample one can include additional regressors such as price in the model. Create sensitivity scenarios around some of the variables such as trend which will make the model good for both forecasting as well as explaining dynamics.  

There is a new package by Rami Krispin [forecastLM](https://github.com/RamiKrispin/forecastLM) - some of the concepts are also convered in his book [Hands-om Time-Series Analysis with R](https://www.amazon.ca/Hands-Time-Analysis-forecasting-visualization-ebook/dp/B07B41P2HZ)

There is a fantastic corresponding blogpost [Introduction to the forecastLM package](https://ramikrispin.github.io/2020/02/introduction-to-the-forecastlm-package/) 



## Long Term Modelling

The key principles of long term forecasting according to [1] include:

+ **Implement a systematic approach** - This includes model driven forecasting. The models are generally grounded in reality and back tested but limited number of assumptions are relaxed and flexed in order to generate new forecasts

+ **Document and Justify** - Formalizing rules and assumptions and systematic approach, Formal documentation and lineage allows for better forecast quality


+ **Systematic Evaluation of Forecasts** - Systematically evaluate forecasts and understand sources of errors and misses


+ **Segregate Forecast and Users** - Don't allow the users of the data to be the forecasters

There are number of approaches one can deploy for long term forecasting


+ **Average of other forecasters** - If available and sufficiently differing forecasts can be used to generate a new forecast. We can remove extreme cases and average number of sufficiently different forecasts to come up with a new forecast. Fair market value curves and forward curves are best estimate of value of an asset at any given time. Therefore, can be used to estimate the future value of an asset. 

+ **Forecasting by Analogy** - We can use analogy of similar situation and similar context to come up with forecasts. This relies on studying historical market dynamics in the same market and cross market. Understand if similar market fundamental prevail what will be the price move. 

+ **Scenario forecasting** - The best approach for forecasting the future since future is generally highly uncertain. Boundary scenarios are generally much easier to forecast. For example "market clearing mechanisms" in commodities markets. Event forecasts can also be used if key events in a market are known (e.g. infrastructure build). Since we can be left with large number of scenarios, it is useful to come up with likelihood of the event occurring. In this case beysian inference as well as statistical methods can be used to get the likelihood of scenarios. In developing market scenarios in depth knowledge of market dynamics is essential. Dimensionality reduction methods such as PCA/PCR are useful to reduce the degrees of freedom in the problem. e.g Forward curve moves can be classified as: Parallel shift, Steepening, and changes in seasonality.  


### References


[1] Github for the training sessions [github](https://github.com/rstudio-conf-2020/time-series-forecasting/blob/master/README.md)

[2] [Forecasting Principles and Practive - Rob J Hyndman and George Athanasopoulos](https://otexts.com/fpp3/)

[3] [Hands-On Time Series Analysis with R: Perform time series analysis and forecasting using R by Rami Krispin] (https://www.amazon.ca/Hands-Time-Analysis-forecasting-visualization-ebook/dp/B07B41P2HZ)




